# People Detector (YOLOv8) — детекция людей на видео
Скрипт детектирует людей на видео, отрисовывает рамки и подписи.

> Ниже — короткая GIF-демонстрация результата и пара ключевых кадров.

<p align="center">
  <img src="assets/demo.gif" alt="Демонстрация результата" width="720">
</p>

<p align="center">
  <p>Пример: плотная толпа</p>
  <img src="assets/result_frame_crowd.png" alt="Пример: плотная толпа" width="50%">
  <p>Пример: хорошо детектируется</p>
  <img src="assets/result_frame_easy.png" alt="Пример: хорошо детектируется" width="100%">
</p>

---

## Структура проекта

```

people-detector/
├─ .venv/                  # виртуальное окружение 
├─ out/
│  └─ crowd_annotated.mp4  # пример выходного видео
├─ assets/                 # изображения и гифка выходного видео
├─ src/
│  ├─ drawing.py           # отрисовка рамок/подписей
│  ├─ infer.py             # загрузка модели и инференс 
│  └─ utils.py             # ввод/вывод видео, сервисные функции
├─ crowd.mp4               # входное видео 
├─ main.py                 # точка входа
├─ requirements.txt        # зависимости
├─ yolov8s.pt              # локальные веса модели YOLOv8
└─ README.md

````

## Возможности

- Предобученная YOLOv8 (Ultralytics) по COCO — класс `person`.
- Тонкие рамки и компактные подписи **над** боксом.
- Настраиваемые параметры из CLI: пороги `--conf/--iou`, частота `--stride`,
  минимальный размер бокса `--min_box`, входной размер `--imgsz`..

---

## Установка

```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
source .venv/bin/activate

pip install -r requirements.txt
````
---

## Запуск

Минимальный запуск (используются значения по умолчанию):

```bash
python main.py
```

Также можно явно указать все ключевые параметры, например:

```bash
python main.py \
  --input crowd.mp4 \
  --output out/crowd_annotated.mp4 \
  --model yolov8s.pt \
  --device auto \
  --conf 0.35 \
  --iou 0.45 \
  --stride 1 \
  --min_box 16 \
  --imgsz 640
```

Посмотреть помощь:

```bash
python main.py --help
```

---

## Параметры CLI 

* `--input` — путь к входному видео (по умолчанию `crowd.mp4`).
* `--output` — путь к выходному видео MP4 (по умолчанию `out/crowd_annotated.mp4`).
* `--model` — веса YOLO (`yolov8n.pt`, `yolov8s.pt`, локальный `*.pt`).
* `--device` — `auto` / `cuda` / `cpu`. `auto` сам выберет устройство.
* `--conf` — порог уверенности [0..1]. Ниже — отброс (больше → чище, меньше → больше находок).
* `--iou` — порог IoU для NMS [0..1]. Выше — меньше слияния соседних боксов.
* `--stride` — детектировать каждый N-й кадр (ускорение на CPU).
* `--min_box` — отбрасывать слишком маленькие боксы (минимальная сторона, px).
* `--imgsz` — входной размер для инференса (px). 640/832/960/1280…
  Больше — лучше для мелких объектов, но медленнее.

---

## Результат

На выходе — видео с рамками класса **person** и подписью `person {score}`, цвет рамки меняется
плавным градиентом **по уверенности** (красный → зелёный).
Выходной FPS и размер совпадают с входными.
---

## Проблемные случаи 

Ниже — кадры, где модель срабатывает хуже. Эти примеры использованы для подбора параметров и планов улучшений.

1. **Ложные срабатывания**   
   <br>
   <p> Картина принимается за человека:</p>
   <img src="assets/issue_false_poster.png" alt="Ложное: картина" width="50%">
   <p> Куртка на стуле принимается за человека:</p>
   <img src="assets/issue_false_jacket.png" alt="Ложное: куртка" width="50%">

2. **Мелкие люди на заднем плане**  
   <p> Далёкие, быстро движущиеся люди не детектируются или “слипаются” из-за NMS.</p>
   <br>
   <img src="assets/issue_small_far.png" alt="Проблема: мелкие дальние люди" width="50%">

**Что делаем дальше:**
- Увеличиваем `--imgsz` (960–1280), понижаем `--conf` (0.25–0.3), поднимаем `--iou` (0.55–0.6).
- Снижаем `--min_box` (6–10) и поднимаем `max_det` (до 1000) в `predict`.
- При необходимости — **тайлинг** (плитки 640×640 с overlap) и/или трекинг между детекциями.
- Долгосрочно — дообучение на похожих сценах (CrowdHuman/свои кадры).
